{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Series 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the parts marked by __'???'__ with approriate explanation, library, object, function, attribute, variable, fixed value, argument or argument value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. understanding td-idf\n",
    "\n",
    "Consider tf-idf score for word i and document j of a corpus. Which of the following statements are correct and which ones are false? Justify your answers.\n",
    "\n",
    "a) tf-idf is lower if word i occurs in many documents\n",
    "\n",
    "b) tf-idf is higher if word i occurs in many documents\n",
    "\n",
    "c) tf-idf does not count how often word i occurs in documents other than document j\n",
    "\n",
    "d) tf-idf does not count how often word i occurs in document j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. dealing with missing values\n",
    "\n",
    "The soybean data can also be found at the [UC Irvine Machine Learning\n",
    "Repository](https://archive.ics.uci.edu/ml/datasets/Soybean+(Large%29). Data were collected to predict disease in 307 soybeans. The 35 features are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.\n",
    "\n",
    "To investigate this dataset, we first import required libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the data set, located in soybean.csv file, and view its first samples\n",
    "# hint: use the read_csv function of pandas\n",
    "\n",
    "df = pd.???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# view the first fe elements of the dataset\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine total number of samples and features\n",
    "\n",
    "num_total_samples = len(df.???)\n",
    "num_total_features = len(df.???)\n",
    "print (num_total_samples, num_total_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "\n",
    "We need to understand what percentage of samples have missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print number of valid entries for each sample\n",
    "# hint : take advantage of the count function\n",
    "\n",
    "num_valid_entries_per_sample = df.???\n",
    "print (num_valid_entries_per_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print number of complete samples\n",
    "\n",
    "num_complete_samples = num_valid_entries_per_sample.tolist().???\n",
    "print(num_complete_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute and print percentage of damaged samples\n",
    "\n",
    "percentage_damaged_samples = 1 - ??? / ???\n",
    "print('Percentage of Damaged Samples:', np.around(100*percentage_damaged_samples,decimals=1), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "\n",
    "We also want to understand if there are particular features that are more likely to be missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count and print number of valid entries for each feature (sorted)\n",
    "\n",
    "num_valid_entries_per_feature = df.count(axis=0).???\n",
    "print(num_valid_entries_per_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there particular features which are more likely to be missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "Now we need to investigate whether the pattern of missing data related to the outcome classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute mean number of valid entires for different outcome classes\n",
    "\n",
    "num_valid_entries_per_sample.groupby(by=???).???.sort_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there some link between the missing and outcome classes? Explain.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "In case of not having an expert to fill out missing values, we want to understand which strategy is more feasible to deal with samples including missing values: (i) sample elimination, (ii) value imputation, and (iii) a hybrid of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do so, we try to investigate in which classes missing values are mostly observed and whether those observations are systematic (e.g. all missing values belong to the same field). Based on those observations, we try to come up with an strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe containing only complete data by droping outcome classes that contain missing values\n",
    "\n",
    "df_complete_data = df.drop(???)\n",
    "df_complete_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verify that the resulting data set is actually complete\n",
    "\n",
    "num_total_entries_complete_data = len(df_complete_data.???)*len(df_complete_data.???)\n",
    "num_total_valid_entries_complete_data = ???(df_complete_data.count().tolist())\n",
    "percentage_damaged_entries_complete_data = ??? - num_total_valid_entries_complete_data/num_total_entries_complete_data\n",
    "print('Percentage of Damaged Samples:', np.around(100*percentage_damaged_entries_complete_data,decimals=1), '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract the samples belonging to the '2-4-d-injury' outcome class from the original data set\n",
    "\n",
    "df_injury_raw = df.???[df.index.???(['2-4-d-injury'])]\n",
    "df_injury_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could such items be eliminated or you would impute the missing values? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the samples belonging to the 'cyst_nematode' outcome class from the original data set\n",
    "\n",
    "df_cyst_nematode_raw = df.???[df.index.???(['cyst-nematode'])]\n",
    "df_cyst_nematode_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could such items be eliminated or you would impute the missing values? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract the samples belonging to the 'herbicide-injury' outcome class from the original data set\n",
    "\n",
    "df_cyst_herbicide_raw = df.???[df.index.???(['herbicide-injury'])]\n",
    "df_cyst_herbicide_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could such items be eliminated or you would impute the missing values? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the samples belonging to the 'diaporthe-pod-&-stem-blight' outcome class from the original data set\n",
    "\n",
    "df_cyst_diaporthe_raw = df.???[df.index.???(['diaporthe-pod-&-stem-blight'])]\n",
    "df_cyst_diaporthe_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could such items be eliminated or you would impute the missing values? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the samples belonging to the 'phytophthora-rot' outcome class from the original data set\n",
    "\n",
    "df_phytophthora_raw = df.???[df.index.???(['phytophthora-rot'])]\n",
    "df_phytophthora_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could such items be eliminated or you would impute the missing values? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " now implement the following strategy: \n",
    "\n",
    " * all samples with more than 24 missing fields will be eliminated\n",
    " * all missing values of 'phytophthora-rot' outcome class will be imputed by the most frequent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dropping those with more than 24 missing fields and print the new shape\n",
    "\n",
    "df.dropna(axis=???, how=???, thresh=???, inplace=???)\n",
    "print (df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imputaton of missing values of 'phytophthora-rot' outcome class\n",
    "# by replacing with 'the most frequent values'\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values=???, strategy=???, axis=???)\n",
    "imp.fit(???)\n",
    "df_no_miss = imp.transform(???)\n",
    "print (df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check whether any missing values remained by checking any shape change\n",
    "\n",
    "test_df = ???.dropna(axis=0, how='any')\n",
    "print (test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there could be different strategies to deal with missing values of a single dataset and usually application-specific knowledge is a deciding factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. data exploration and outlier detection\n",
    "\n",
    "The UC Irvine Machine Learning Repository contains a [data set related to glass identification]( https://archive.ics.uci.edu/ml/datasets/Glass+Identification ). The data consist of 214 glass samples labeled as one of seven glass categories. There are nine features, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe. \n",
    "\n",
    "To investigate this dataset for our purposes, we first import required libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sbn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data set\n",
    "df = pd.read_csv('glass.csv', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using visualizations, we want to explore the pairwise relationships between features as well as distribution of each feature. One conveient way of gaining a first impression of the data is by means of the 'pairplot' functionality offered in the 'seaborn' library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize distribution of features and pairwise relationship using seaborn pairplot\n",
    "sbn.pairplot(***)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret each of your graphs, whether on the diagonal line or off the diagonal line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to calculate the correlation of each feature with the outcome class as well as with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute and print the pairwise correlation between the columns in data frame\n",
    "\n",
    "correlations = ***.corr()\n",
    "correlations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the interpretation of each number in the above table? How they are calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** As the outcome classes are categorical, the correlation between them and the continuous explanatory variables needs to be interpreted with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "considering feature distributions, do there appear to be any outliers in the data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. data transformation techniques\n",
    "\n",
    "Use different methods to transform the following values of feature 'age':\n",
    "\n",
    " 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70\n",
    " \n",
    " We are going to apply different transformation on this dataset. first lets define our dataset in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# define dataset\n",
    "X = np.array([[13], [15], [16], [16], [19], [20], [20], [21], [22], [22], [25], [25], [25], [25],\\\n",
    "              [30], [33], [33], [35], [35], [35], [35], [36], [40], [45], [46], [52], [70]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "\n",
    "We need to apply min-max standardization on this dataset by setting min = 0 and max = 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scale data to attain minimum value 0 and maximum value 1\n",
    "\n",
    "min_max_scaler = preprocessing.???\n",
    "X_PartA = min_max_scaler.???(X)\n",
    "print(X_PartA.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "\n",
    "Next we want to standardize our dataset to have mean zero and standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# standardize data (i.e. scale to mean zero and standard deviation one)\n",
    "\n",
    "X_PartB = preprocessing.???(X)\n",
    "print('sample mean:', X_PartB.???())\n",
    "print('sample stddev:', X_PartB.???())\n",
    "print(X_PartB.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "Now we want to normalize our data using L1 and L2 norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize using L1-norm\n",
    "\n",
    "X_PartC_L1 = preprocessing.???(X.ravel(), norm=???)\n",
    "print('1-norm of sample vector:', np.linalg.norm(X_PartC_L1,1,axis=1))\n",
    "print(X_PartC_L1.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# normalize using L2-norm\n",
    "\n",
    "X_PartC_L2 = preprocessing.???(X.ravel(), norm=???)\n",
    "print('2-norm of sample vector:', np.linalg.norm(X_PartC_L2,2,axis=1))\n",
    "print(X_PartC_L2.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Next we need to partition data into 3 buckets using equal-frequency and equal-width partitioning and labels each category to 'young', 'middle' and 'old'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate equal frequency partition\n",
    "\n",
    "PartD_eqFreq = pd.???(X.ravel(), ???, labels=[\"young\",\"middle\",\"old\"])\n",
    "print(PartD_eqFreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate equal width partition\n",
    "\n",
    "PartD_eqWidth = pd.???(X.ravel(), ???, labels=[\"young\",\"middle\",\"old\"])\n",
    "print(PartD_eqWidth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "\n",
    "Finally we want to encode the categorical feature obtained in part (d) using one hot encoding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re-generate equal frequency partition, but with numerical values of categories\n",
    "\n",
    "PartE_eqFreq = pd.???(X.ravel(), ???, labels=[1,2,3])\n",
    "print(PartE_eqFreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate, fit and apply One Hot Encoder\n",
    "\n",
    "enc = preprocessing.???()\n",
    "enc.???((np.reshape(PartE_eqFreq.get_values(),(27,1))))\n",
    "print(enc.transform([[???]]).toarray().ravel()) # encoding 'young'\n",
    "print(enc.transform([[???]]).toarray().ravel()) # encoding 'middle'\n",
    "print(enc.transform([[???]]).toarray().ravel()) # encoding 'old'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# re-generate equal width partition, but with numerical values of categories\n",
    "\n",
    "PartE_eqWidth = pd.???(X.ravel(), ???, labels=[1,2,3])\n",
    "print(PartE_eqWidth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate, fit and apply One Hot Encoder\n",
    "\n",
    "enc = preprocessing.???()\n",
    "enc.???((np.reshape(PartE_eqWidth.get_values(),(27,1))))\n",
    "print(enc.transform([[???]]).toarray().ravel()) # encoding 'young'\n",
    "print(enc.transform([[???]]).toarray().ravel()) # encoding 'middle'\n",
    "print(enc.transform([[???]]).toarray().ravel()) # encoding 'old'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. feature extraction of text documents using bag of words\n",
    "\n",
    "Consider the following corpus:\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "             \n",
    "Using sklearn feature extraction facilities, we want to vectorize the above text. First lets define the documents and stop words list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define documents and stop list\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\", \\\n",
    "             \"A survey of user opinion of computer system response time\", \\\n",
    "             \"The EPS user interface management system\", \\\n",
    "             \"System and human system engineering testing of EPS\", \\\n",
    "             \"Relation of user perceived response time to error measurement\", \\\n",
    "             \"The generation of random binary unordered trees\", \\\n",
    "             \"The intersection graph of paths in trees\", \\\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\", \\\n",
    "             \"Graph minors A survey\"]\n",
    "stoplist = set('for a of the and to in'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "\n",
    "First tokenize the corpus by applying normalization and stop word removal (use feature_extraction.text.CountVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create vectorizer instance including the stop list\n",
    "\n",
    "vectorizer = ???(stop_words=???)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit vectorizer, carry out vectorization and display results\n",
    "\n",
    "vectorizer.fit(???)\n",
    "documents_vec = vectorizer.???(documents)\n",
    "print(documents_vec) # sparse matrix representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "\n",
    "Next obtain the tf-idf value for each token of each sentence in the corpus (use sklearn.feature_extraction.text.TfidfTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create transformer instance to later compute TF-IDF values\n",
    "\n",
    "transformer = ???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit transformer, compute TF-IDF scores and display results\n",
    "\n",
    "transformer.fit(???)\n",
    "documents_tfidf = transformer.???(documents_vec)\n",
    "print(documents_tfidf) # sparse matrix representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "Finally we create a simple pipeline to apply both steps at once (using sklearn.pipeline.Pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up pipeline to sequentially carry out vectorization and transformation\n",
    "\n",
    "pipe = Pipeline([('vectorize', ???(stop_words=???)), ('compute_tfidf', ???())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit pipeline, then carry out vectorization and transformation in one shot\n",
    "\n",
    "pipe.fit(???)\n",
    "documents_tfidf = pipe.transform(???)\n",
    "print(documents_tfidf) # sparse matrix representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. stemming of text documents\n",
    "\n",
    "Feature extraction facilities of sklearn do not provide stemming. Modify your pipeline in the previous question to incorporate stemming facilities of nltk (use nltk.stem.snowball.EnglishStemmer) and regenerate vectorized form of the corpus with new features and tf-idf values. \n",
    "\n",
    "Hint: you can configure your CountVectorizer object as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words)\n",
    "print(stem_vectorizer.fit_transform(['with power comes great responsibilities']))\n",
    "print(stem_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.snowball import ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure CountVectorizer to include nltk-stemmer as suggested in the question statement\n",
    "\n",
    "stemmer = ???()\n",
    "analyzer = CountVectorizer().???()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.???(???) for ??? in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up pipeline with the modified CountVectorizer\n",
    "\n",
    "pipe_modified = Pipeline([('vectorize_stem', ???(stop_words=???, analyzer=???)),\\\n",
    "                          ('compute_tfidf', ???())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit modified pipeline, then carry out vectorization and transformation in one shot and print the results\n",
    "\n",
    "pipe_modified.???(documents)\n",
    "documents_tfidf = pipe_modified.???(documents)\n",
    "print(documents_tfidf) # sparse matrix representation\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
